{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kelompok4_DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPK9VPZrP1gLLScaCVu5H4e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cicaazizahfit/Tugas1DeepLearning/blob/master/Kelompok4_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pter3PdqYc5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as f\n",
        "\n",
        "\n",
        "def calc_loss_acc(labels, logits):\n",
        "    \"\"\"Function to compute loss value. Here, we used cross entropy \n",
        "    Args:\n",
        "        logits: 4D tensor. output tensor from segnet model, which is the output of softmax\n",
        "        labels: true labels tensor\n",
        "    Returns:\n",
        "        loss (cross_entropy_mean), accuracy, predicts(logits with softmax) \n",
        "    \"\"\"\n",
        "    # calc cross entropy mean  cross_entropy\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='cross_entropy')\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
        "    tf.summary.scalar(name='loss', tensor=cross_entropy_mean)\n",
        "\n",
        "\n",
        "    predicts = tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1))\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(predicts, dtype=tf.float32))\n",
        "    tf.summary.scalar(name='accuracy', tensor=accuracy)\n",
        "\n",
        "    return cross_entropy_mean, accuracy, predicts\n",
        "\n",
        "\n",
        "def train_op(total_loss, global_steps, base_learning_rate, option='Adam'):\n",
        "    \"\"\"This function defines train optimizer \n",
        "    Args:\n",
        "        total_loss: the loss value\n",
        "        global_steps: global steps is used to track how many batch had been passed. In the training process, the initial value for global_steps = 0, here  \n",
        "        global_steps=tf.Variable(0, trainable=False). then after one batch of images passed, the loss is passed into the optimizer to update the weight, then the global \n",
        "        step increased by one.\n",
        "        base_learning_rate: default value 0.1\n",
        "    Returns:\n",
        "        the train optimizer\n",
        "    \"\"\"\n",
        "\n",
        "    # base_learning_rate = 0.01\n",
        "    # get update operation\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "\n",
        "        if option == 'Adam':\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=base_learning_rate)\n",
        "            print(\"Running with Adam Optimizer with learning rate:\", base_learning_rate)\n",
        "        elif option == 'SGD':\n",
        "            # base_learning_rate = 0.01\n",
        "            learning_rate_decay = tf.train.exponential_decay(base_learning_rate, global_steps, 1000, 0.0005)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate_decay)\n",
        "            print(\"Running with SGD Optimizer with learning rate:\", learning_rate_decay)\n",
        "        else:\n",
        "            raise ValueError('Optimizer is not recognized')\n",
        "\n",
        "        grads = optimizer.compute_gradients(total_loss, var_list=tf.trainable_variables())\n",
        "        training_op = optimizer.apply_gradients(grads, global_step=global_steps)\n",
        "\n",
        "    return training_op\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqtRkBEBe-aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def max_pooling(inputs, ksize, stride, padding, name):\n",
        "    \"\"\"Create max pooling layer \n",
        "    Args:\n",
        "        inputs: float32 4D tensor\n",
        "        ksize: a tuple of 2 int with (kernel_height, kernel_width)\n",
        "        stride: a tuple\n",
        "        padding: string. padding mode 'SAME', '\n",
        "        name: string\n",
        "        \n",
        "    Returns:\n",
        "        4D tensor of [batch_size, height, width, channels]\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        value = tf.nn.max_pool(inputs, ksize=[1, ksize[0], ksize[1], 1], strides=[1, stride[0], stride[1], 1], padding=padding, name=scope.name)\n",
        "    return value\n",
        "\n",
        "def dropout(inputs, keep_prob, name):\n",
        "    \"\"\"Dropout layer\n",
        "    Args:\n",
        "        inputs: float32 4D tensor \n",
        "        keep_prob: the probability of keep training sample\n",
        "        name: layer name to define\n",
        "    Returns:\n",
        "        4D tensor of [batch_size, height, width, channels]\n",
        "    \"\"\"\n",
        "    \n",
        "    return tf.nn.dropout(inputs, rate=(1-keep_prob), name=name)\n",
        "\n",
        "\n",
        "def norm(inputs, radius=4, name=None):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        value = tf.nn.lrn(inputs, depth_radius=radius, bias=1.0, alpha=1e-4, beta=0.75, name=name)\n",
        "    return value\n",
        "\n",
        "# def batch_norm(inputs, name):\n",
        "#     \"\"\"batch normalization layer\n",
        "\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "def conv2d(inputs, shape, padding, name):\n",
        "    \"\"\"Create convolution 2D layer\n",
        "    Args:\n",
        "        inputs: float32. 4D tensor\n",
        "        shape: the shape of kernel \n",
        "        padding: string. padding mode 'SAME',\n",
        "        name: corressponding layer's name\n",
        "    Returns:\n",
        "        Output 4D tensor\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        # get weights value and record a summary protocol buffer with a histogram\n",
        "        weights = tf.get_variable('weights', shape=shape, initializer=tf.initializers.he_normal())\n",
        "        tf.summary.histogram(scope.name + 'weights', weights)\n",
        "\n",
        "        # get biases value and record a summary protocol buffer with a histogram\n",
        "        biases = tf.get_variable('biases', shape=shape[3], initializer=tf.initializers.random_normal())\n",
        "        tf.summary.histogram(scope.name + 'biases', biases)\n",
        "        # compute convlotion W * X + b, activiation function relu function\n",
        "        outputs = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding=padding)\n",
        "        outputs = tf.nn.bias_add(outputs, biases)\n",
        "        outputs = tf.nn.relu(outputs, name=scope.name + 'relu')\n",
        "    return outputs\n",
        "\n",
        "def fc(inputs, shape, name):\n",
        "    \"\"\"Create fully collection layer \n",
        "    Args:\n",
        "        inputs: Float32. 2D tensor with shape [batch, input_units]\n",
        "        shape: Int. a tuple with [num_inputs, num_outputs]\n",
        "        name: sring. layer name\n",
        "    Returns:\n",
        "        Outputs fully collection tensor\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        weights = tf.get_variable('weights', shape = [shape[0], shape[1]], initializer=tf.initializers.he_normal())\n",
        "        biases = tf.get_variable('biases', shape = [shape[1]], initializer=tf.initializers.random_normal())\n",
        "        # outputs = tf.nn.xw_plus_b(inputs, weights, biases, name = scope.name)\n",
        "        outputs = tf.add(tf.matmul(inputs, weights), biases, name=scope.name)\n",
        "\n",
        "    return tf.nn.relu(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7o7MxjIfQrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def alexnet(inputs, num_classes, keep_prob):\n",
        "    \"\"\"Create alexnet model\n",
        "    \"\"\"\n",
        "    x = tf.reshape(inputs, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # first conv layer, downsampling layer, and normalization layer\n",
        "    conv1 = conv2d(x, shape=(11, 11, 1, 96), padding='SAME', name='conv1')\n",
        "    pool1 = max_pooling(conv1, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool1')\n",
        "    norm1 = norm(pool1, radius=4, name='norm1')\n",
        "\n",
        "    # second conv layer\n",
        "    conv2 = conv2d(norm1, shape=(5, 5, 96, 256), padding='SAME', name='conv2')\n",
        "    pool2 = max_pooling(conv2, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool2')\n",
        "    norm2 = norm(pool2, radius=4, name='norm2')\n",
        "\n",
        "    # 3rd conv layer\n",
        "    conv3 = conv2d(norm2, shape=(3, 3, 256, 384), padding='SAME', name='conv3')\n",
        "    # pool3 = max_pooling(conv3, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool3')\n",
        "    norm3 = norm(conv3, radius=4, name='norm3')\n",
        "\n",
        "    # 4th conv layer\n",
        "    conv4 = conv2d(norm3, shape=(3, 3, 384, 384), padding='SAME', name='conv4')\n",
        "\n",
        "    # 5th conv layer\n",
        "    conv5 = conv2d(conv4, shape=(3, 3, 384, 256), padding='SAME', name='conv5')\n",
        "    pool5 = max_pooling(conv5, ksize=(2, 2), stride=(2, 2), padding='SAME', name='pool5')\n",
        "    norm5 = norm(pool5, radius=4, name='norm5')\n",
        "\n",
        "    # first fully connected layer\n",
        "    fc1 = tf.reshape(norm5, shape=(-1, 4*4*256))\n",
        "    fc1 = fc(fc1, shape=(4*4*256, 4096), name='fc1')\n",
        "    fc1 = dropout(fc1, keep_prob=keep_prob, name='dropout1')\n",
        "\n",
        "    fc2 = fc(fc1, shape=(4096, 4096), name='fc2')\n",
        "    fc2 = dropout(fc2, keep_prob=keep_prob, name='dropout2')\n",
        "\n",
        "    # output logits value\n",
        "    with tf.variable_scope('classifier') as scope:\n",
        "        weights = tf.get_variable('weights', shape=[4096, num_classes], initializer=tf.initializers.he_normal())\n",
        "        biases = tf.get_variable('biases', shape=[num_classes], initializer=tf.initializers.random_normal())\n",
        "        # define output logits value\n",
        "        logits = tf.add(tf.matmul(fc2, weights), biases, name=scope.name + '_logits')\n",
        "\n",
        "    return logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UOVXeGggsNI",
        "colab_type": "code",
        "outputId": "37da5fd0-09c1-44b2-b12b-955e3eb11c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "      delattr(flags.FLAGS,name)\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "flags.DEFINE_integer('valid_steps', 11, 'The number of validation steps ')\n",
        "\n",
        "flags.DEFINE_integer('max_steps', 1001, 'The number of maximum steps for traing')\n",
        "\n",
        "flags.DEFINE_integer('batch_size', 128, 'The number of images in each batch during training')\n",
        "\n",
        "flags.DEFINE_float('base_learning_rate', 0.0001, \"base learning rate for optimizer\")\n",
        "\n",
        "flags.DEFINE_integer('input_shape', 784, 'The inputs tensor shape')\n",
        "\n",
        "flags.DEFINE_integer('num_classes', 10, 'The number of label classes')\n",
        "\n",
        "flags.DEFINE_string('save_dir', './outputs', 'The path to saved checkpoints')\n",
        "\n",
        "flags.DEFINE_float('keep_prob', 0.75, \"the probability of keeping neuron unit\")\n",
        "\n",
        "flags.DEFINE_string('tb_path', './tb_logs/', 'The path points to tensorboard logs ')\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(FLAGS):\n",
        "    \"\"\"Training model\n",
        "    \"\"\"\n",
        "    valid_steps = FLAGS.valid_steps\n",
        "    max_steps = FLAGS.max_steps\n",
        "    batch_size = FLAGS.batch_size\n",
        "    base_learning_rate = FLAGS.base_learning_rate\n",
        "    input_shape = FLAGS.input_shape  # image shape = 28 * 28\n",
        "    num_classes = FLAGS.num_classes\n",
        "    keep_prob = FLAGS.keep_prob\n",
        "    save_dir = FLAGS.save_dir\n",
        "    tb_path = FLAGS.tb_path \n",
        "\n",
        "    train_loss, train_acc = [], []\n",
        "    valid_loss, valid_acc = [], []\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    # define default tensor graphe \n",
        "    with tf.Graph().as_default():\n",
        "        images_pl = tf.placeholder(tf.float32, shape=[None, input_shape])\n",
        "        labels_pl = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
        "\n",
        "        # define a variable global_steps\n",
        "        global_steps = tf.Variable(0, trainable=False)\n",
        "\n",
        "        # build a graph that calculate the logits prediction from model\n",
        "        logits = alexnet(images_pl, num_classes, keep_prob)\n",
        "\n",
        "        loss, acc, _ = calc_loss_acc(labels_pl, logits)\n",
        "\n",
        "        # build a graph that trains the model with one batch of example and updates the model params \n",
        "        training_op = train_op(loss, global_steps, base_learning_rate)\n",
        "\n",
        "        # define the model saver\n",
        "        saver = tf.train.Saver(tf.global_variables())\n",
        "        \n",
        "        # define a summary operation \n",
        "        summary_op = tf.summary.merge_all()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            # print(sess.run(tf.trainable_variables()))\n",
        "            # start queue runners\n",
        "            coord = tf.train.Coordinator()\n",
        "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "            train_writter = tf.summary.FileWriter(tb_path, sess.graph)\n",
        "\n",
        "            # start training\n",
        "            for step in range(100):\n",
        "                # get train image / label batch\n",
        "                train_image_batch, train_label_batch = mnist.train.next_batch(batch_size)\n",
        "\n",
        "                train_feed_dict = {images_pl: train_image_batch, labels_pl: train_label_batch}\n",
        "\n",
        "                _, _loss, _acc, _summary_op = sess.run([training_op, loss, acc, summary_op], feed_dict = train_feed_dict)\n",
        "\n",
        "                # store loss and accuracy value\n",
        "                train_loss.append(_loss)\n",
        "                train_acc.append(_acc)\n",
        "                print(\"Iteration \" + str(step) + \", Mini-batch Loss= \" + \"{:.6f}\".format(_loss) + \", Training Accuracy= \" + \"{:.5f}\".format(_acc))\n",
        "\n",
        "                if step % 100 == 0:\n",
        "                    _valid_loss, _valid_acc = [], []\n",
        "                    print('Start validation process')\n",
        "\n",
        "                    for step in range(valid_steps):\n",
        "                        valid_image_batch, valid_label_batch = mnist.test.next_batch(batch_size)\n",
        "\n",
        "                        valid_feed_dict = {images_pl: valid_image_batch, labels_pl: valid_label_batch}\n",
        "\n",
        "                        _loss, _acc = sess.run([loss, acc], feed_dict = valid_feed_dict)\n",
        "\n",
        "                        _valid_loss.append(_loss)\n",
        "                        _valid_acc.append(_acc)\n",
        "\n",
        "                    valid_loss.append(np.mean(_valid_loss))\n",
        "                    valid_acc.append(np.mean(_valid_acc))\n",
        "\n",
        "                    print(\"Iteration {}: Train Loss {:6.3f}, Train Acc {:6.3f}, Val Loss {:6.3f}, Val Acc {:6.3f}\".format(step, train_loss[-1], train_acc[-1], valid_loss[-1], valid_acc[-1]))\n",
        "            \n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'train_loss'), train_loss)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'train_acc'), train_acc)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'valid_loss'), valid_loss)\n",
        "            np.save(os.path.join(save_dir, 'accuracy_loss', 'valid_acc'), valid_acc)\n",
        "            checkpoint_path = os.path.join(save_dir, 'model', 'model.ckpt')\n",
        "            saver.save(sess, checkpoint_path, global_step=step)\n",
        "\n",
        "            coord.request_stop()\n",
        "            coord.join(threads)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    train(FLAGS)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-ec607f8942a1>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ec607f8942a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mdelattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'flags' is not defined"
          ]
        }
      ]
    }
  ]
}